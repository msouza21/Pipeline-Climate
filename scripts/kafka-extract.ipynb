{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config APIs\n",
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "script_dir = os.getcwd()\n",
    "config_path = (script_dir + '/pipeline-climate/ignore/config.json')\n",
    "\n",
    "with open(config_path) as config_file:\n",
    "    config = json.load(config_file)\n",
    "    APIs = config.get('APIs').rstrip('/')\n",
    "\n",
    "env_path = os.path.join(APIs, \".env\")\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "api_keys ={\n",
    "    \"openweather\": os.getenv(\"OPENWEATHER_API_KEY\", \"\"),\n",
    "    \"weatherapi\": os.getenv(\"WEATHERAPI_API_KEY\", \"\"),\n",
    "    \"weatherbit\": os.getenv(\"WEATHERBIT_API_KEY\", \"\"),\n",
    "    \"openaq\": os.getenv(\"OPENAQ_API_KEY\", \"\"),\n",
    "    \"bigdata\": os.getenv(\"BIGDATA_API_KEY\", \"\"),\n",
    "}\n",
    "\n",
    "api_configs = {\n",
    "    \"openweather\": {\n",
    "        \"url\": \"http://api.openweathermap.org/data/2.5/weather\",\n",
    "        \"params\": {\n",
    "            \"q\": \"\", \n",
    "            \"appid\": api_keys[\"openweather\"],\n",
    "            \"units\": \"metric\"\n",
    "        }\n",
    "    },\n",
    "    \"weatherapi\": {\n",
    "        \"url\": \"http://api.weatherapi.com/v1/current.json\",\n",
    "        \"params\": {\n",
    "            \"q\": \"\",\n",
    "            \"key\": api_keys[\"weatherapi\"]\n",
    "        }\n",
    "    },\n",
    "    \"weatherbit\": {\n",
    "        \"url\": \"https://api.weatherbit.io/v2.0/current\",\n",
    "        \"params\": {\n",
    "            \"city\": \"\"  ,\n",
    "            \"key\": api_keys[\"weatherbit\"]\n",
    "        }\n",
    "    },\n",
    "    \"openaq\": {\n",
    "        \"url\": \"https://api.openaq.org/v2/locations\",\n",
    "        \"params\": {\n",
    "            \"city\": \"\",\n",
    "            \"key\": api_keys[\"openaq\"]\n",
    "        }\n",
    "    },\n",
    "    \"bigdata\": {\n",
    "        \"url\": \"https://api.bigdatacloud.net/data/reverse-geocode-client\",\n",
    "        \"params\": {\n",
    "            \"latitude\": \"\",\n",
    "            \"longitude\": \"\",\n",
    "            \"key\": api_keys[\"bigdata\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "param_info = {\n",
    "    'q': 'location'\n",
    "}\n",
    "\n",
    "#Reading data \n",
    "def read_api_data(api_name, *args):\n",
    "    \"\"\"Request of data based in wanted API\"\"\"\n",
    "    if api_name in api_configs:\n",
    "        config = api_configs[api_name]\n",
    "        \n",
    "        params = config[\"params\"].copy()\n",
    "        params_keys = list(params.keys())\n",
    "        \n",
    "        for i, key in enumerate(params_keys):\n",
    "            if i < len(args):\n",
    "                params[key] = args[i]\n",
    "\n",
    "        response = requests.get(config[\"url\"], params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Error to request data from API: {api_name}: {response.status_code}\")\n",
    "    else:\n",
    "        print(f\"API '{api_name}' not found\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kafka config\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "from confluent_kafka.admin import AdminClient\n",
    "\n",
    "prod_conf = {\n",
    "    'bootstrap.servers': 'localhost:9092, localhost:9094',\n",
    "    'client.id': 'py-producer'\n",
    "}\n",
    "\n",
    "prod = Producer(**prod_conf) \n",
    "\n",
    "def msg_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f'Msg error: {msg} - {err}')\n",
    "    else:\n",
    "        print(f'Msg delivered to topic: {msg.topic()}, partition: [{msg.partition()}]')\n",
    "\n",
    "\n",
    "admin_client = AdminClient({'bootstrap.servers': 'localhost:9092, localhost:9094'})\n",
    "\n",
    "message_count = 0\n",
    "msg_limit = 50\n",
    "\n",
    "\n",
    "def create_partitions(topic, partition_count):\n",
    "    fs = admin_client.alter_partitions([{\n",
    "        'topic':topic,\n",
    "        'new_count': partition_count\n",
    "    }])\n",
    "    for topic, f in fs.items():\n",
    "        try:\n",
    "            f.result()\n",
    "            print(f'Partitions from topic: {topic} increased to: {partition_count}.')\n",
    "        except Exception as e:\n",
    "            print(f'Error in alter partitions from topic {topic}: {str(e)}.')\n",
    "\n",
    "def get_partitions(topic):\n",
    "    topic_metadata = admin_client.list_topics(topic)\n",
    "    if topic_metadata.topics.get(topic):\n",
    "        return len(topic_metadata.topics[topic].partitions)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data from openaq\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "#Code execution\n",
    "def produce_msg(topic, data):\n",
    "    global message_count\n",
    "\n",
    "    js_data = json.dumps(data).encode('utf-8')\n",
    "    try:\n",
    "        prod.produce(topic, key=str(data.get('id', 'unknown')), value = js_data)\n",
    "        prod.poll(0)\n",
    "        message_count += 1\n",
    "\n",
    "        if message_count >= msg_limit:\n",
    "            current = get_partitions(topic)\n",
    "            create_partitions(topic, current + 1)\n",
    "            message_count = 0\n",
    "    except Exception as e:\n",
    "        print(f'Error producing message: {str(e)}')\n",
    "        \n",
    "#def user_input():\n",
    "\n",
    "            \n",
    "def main():\n",
    "    while True:\n",
    "        options = [k for k,v in api_configs.items()]\n",
    "        api_name = input(f\"Options: {options}\\n\\nWrite the name API wanted or type 'quit' to exit: \").strip().lower()\n",
    "\n",
    "        if api_name == 'quit'.lower():\n",
    "            print(f'Exiting...')\n",
    "            break\n",
    "\n",
    "        if api_name in api_configs:\n",
    "            params = []\n",
    "            ocult_params = [\"appid\",\"key\",\"units\"]\n",
    "            for param in api_configs[api_name][\"params\"]:\n",
    "                if param not in ocult_params:\n",
    "                    param_desc = param_info.get(param, param)\n",
    "                    param_input = input(f\"Write the {param_desc}\").strip()\n",
    "                    params.append(param_input)\n",
    "        else:\n",
    "            print(f'API not recognized: {api_name}')\n",
    "            continue\n",
    "        data = read_api_data(api_name, *params)\n",
    "\n",
    "        if data:\n",
    "            print(f'Getting data from {api_name}')\n",
    "            global topic\n",
    "            topic = api_name\n",
    "            produce_msg(topic, data)\n",
    "        prod.flush()\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AWS S3 storage\n",
    "import json\n",
    "import os\n",
    "import boto3\n",
    "from confluent_kafka import Consumer, KafkaException\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "cons_config = {\n",
    "    'bootstrap.servers': 'localhost:9092, localhost:9094',\n",
    "    'group.id': 's3-consumer',\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "\n",
    "cons = Consumer(cons_config)\n",
    "cons.subscribe([topic])\n",
    "\n",
    "\n",
    "script_dir = os.getcwd()\n",
    "config_path = (script_dir + '/pipeline-climate/ignore/config.json')\n",
    "\n",
    "with open(config_path) as config_file:\n",
    "    config = json.load(config_file)\n",
    "    aws = config.get('AWS').rstrip('/')\n",
    "env_file = os.path.join(aws,'.env')\n",
    "load_dotenv(dotenv_path=env_file)\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "    aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "    region_name = os.getenv('AWS_REGION_NAME') \n",
    ")\n",
    "\n",
    "def upload_s3(file_name, data):\n",
    "    bucket_name = 's3bucketsz'\n",
    "\n",
    "    try:\n",
    "        s3_client.put_object(Bucket=bucket_name, Key=file_name, Body=data)\n",
    "        print(f'Uploading {file_name} to S3')\n",
    "    except Exception as e:\n",
    "        print(f'Error uploading {file_name} to S3: {str(e)}')\n",
    "\n",
    "def consumer_msg():\n",
    "    i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            msg = cons.poll(1.0)\n",
    "            if msg is None:\n",
    "                continue\n",
    "            if msg.error():\n",
    "                raise KafkaException(msg.error())\n",
    "            \n",
    "            file_name = f'data/{topic}-{i+1}.json'\n",
    "\n",
    "            data = msg.value().decode('utf-8')\n",
    "\n",
    "            upload_s3(file_name, data)\n",
    "            i += 1\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f'Error in upload to S3: {str(e)}')\n",
    "\n",
    "    cons.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    consumer_msg()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
